# Import list.

import csv

# Import the list of mutually independent sets generated by the SPN and Pre-Process
raw_independency_file=open("Independent_Sets.txt","r")
raw_rows=raw_independency_file.read().split('\n')
raw_rows=raw_rows[1:]
string_lists=[]

# Obtain list of lists of strings.
for i in raw_rows:
    j=i.split(',')
    j=j[1:]
    string_lists.append(j)

integer_lists=[]

# Convert to Integers
for k in string_lists:
    row=[]
    for l in k:
        row.append(int(l))
    integer_lists.append(row)

# Label Protected Integer.
protected_integer=53

# Isolate list of variables which exhibit a strong correlation with the protected attribute according to the initial SPN list.
for indep_set in integer_lists:
    if protected_integer in indep_set:
        key_set=indep_set

print(key_set)

# From length of each attribute, generate another list giving indices of attribute boundaries.
list_of_lengths=[4,10,5,10,10,5,5,4,1,3,4,4,10,3,3,4,4,2,1,1,1]
cumulative=[0]
for i,j in enumerate(list_of_lengths):
    cumulative.append(cumulative[i]+j)
print(cumulative)

attribute_list=[]
attribute_range=[]

# Loop over all attribute components in the key set, and append to a list both that component and all other components contained in that attribute.
for attribute_component in key_set:
    print(attribute_component)
    for attribute, threshold in enumerate(cumulative):
        if (attribute_component<threshold) and (attribute_component>=cumulative[attribute-1]):
            attribute_list.append([i for i in range(cumulative[attribute-1],threshold)])
            attribute_range.append([cumulative[attribute-1],threshold])

# Create unique list of attribute components (previous one could contain multiple copies of same attribute compoenent.
def unique_list_of_lists(list,boolean_element):
    unique_list=[]
    for i in list:
        if boolean_element:
            for j in i:
                if j not in unique_list:
                    unique_list.append(j)
        else:
            if i not in unique_list:
                unique_list.append(i)
    return unique_list


unique_list=unique_list_of_lists(attribute_list,True)
unique_range=unique_list_of_lists(attribute_range,False)

print(attribute_list)

print(unique_list)
print(unique_range)



# Generate cumulative list of ranges (both in context of original block and new block).
old_block_cumulative=sorted(unique_range)
print(old_block_cumulative)

new_block_cumulative=[0]
for scope in old_block_cumulative:
    new_block_cumulative.append(new_block_cumulative[-1]+scope[1]-scope[0])
print("new_block_cumulative")

print(new_block_cumulative)



# We can now import the original preprocessed data and isolate the section we want to keep when training a sub SPN.

# Import, split into lists of strings, remove final (blank) column
original_train_file=open("germancredit.ts.data","r")
original_val_file=open("germancredit.valid.data","r")
original_test_file=open("germancredit.test.data","r")
print(original_test_file)
train_rows=original_train_file.read().split('\n')[:-1]
val_rows=original_val_file.read().split('\n')[:-1]
test_rows=original_test_file.read().split('\n')[:-1]

print(len(train_rows))

# Generate Integer arrays for all 3 data sets.

train_data=[]
for row in train_rows:
    element_row=row.split(',')
    new_row=[int(element) for element in element_row]
    train_data.append(new_row)


val_data=[]
for row in val_rows:
    element_row=row.split(',')
    new_row=[]
    for element in element_row:
        new_row.append(int(element))
    val_data.append(new_row)

test_data=[]
for row in test_rows:
    element_row=row.split(',')
    new_row=[]
    for element in element_row:
        new_row.append(int(element))
    test_data.append(new_row)

# function to isolate just columns whose indices are the same as the attribute components appearing in cols_to_keep.
def isolate(data, cols_to_keep):
    new_data=[]
    for instance in data:
        temp_list=[]
        for label, variable in enumerate(instance):
            if label in cols_to_keep:
                temp_list.append(variable)
        new_data.append(temp_list)
    return new_data

# Perform function on existing arrays.
dependent_train=isolate(train_data,unique_list)
dependent_val=isolate(val_data,unique_list)
dependent_test=isolate(test_data,unique_list)

# Write these to files so we can train a sub-SPN. 

dependentTrainFile="germanCreditDependent.ts.data"
dependentValFile="germanCreditDependent.valid.data"
dependentTestFile="germanCreditDependent.test.data"

with open(dependentTrainFile,"w") as output:
    writer = csv.writer(output,lineterminator='\n')
    writer.writerows(dependent_train)

with open(dependentValFile,"w") as output:
    writer = csv.writer(output,lineterminator='\n')
    writer.writerows(dependent_val)

with open(dependentTestFile,"w") as output:
    writer = csv.writer(output,lineterminator='\n')
    writer.writerows(dependent_test)


print(len(unique_list))
# Separate out independent lists.

# Separate Data Accordingly

# Write to New File.

# Now generate the Inference Queries.

queries=[]
evidence_male=[]
evidence_female=[]

total_number=new_block_cumulative[-1]

for inference_instance in range(total_number):
    temporary=["*" for j in range(total_number)]
    for position,threshold in enumerate(new_block_cumulative):
        
        # Only perform procedure if the inference_instance is in the range of the attribute we're currently considering
        if (inference_instance<threshold) and (inference_instance>=new_block_cumulative[position-1]):
            
            # Procedure for binary variables (must do [*,*,*,0,*] and [*,*,*,1,*]).
            if (threshold-new_block_cumulative[position-1])==1:
                # Set up two queries, one with a 0 in the relevant position and the other with a one.
                temporary_1=["*" for j in range(total_number)]
                temporary_2=["*" for j in range(total_number)]
                temporary_1[inference_instance]=0
                temporary_2[inference_instance]=1
                
                # Append to list of queries
                queries.append(temporary_1)
                queries.append(temporary_2)
                
                # Now allocate an evidence query (1 or 0 in protected attribute column
                temporary_evidence_1=["*" for j in range(total_number)]
                temporary_evidence_F1=["*" for j in range(total_number)]
                temporary_evidence_1[5]=0
                temporary_evidence_F1[5]=1
                
                # Append to evidence
                evidence_male.append(temporary_evidence_1)
                evidence_male.append(temporary_evidence_1)
                evidence_female.append(temporary_evidence_F1)
                evidence_female.append(temporary_evidence_F1)

        # Procedure for one-hot encoded variables (e.g. [*,*,0,0,1,0,*,*]
            else:
                temp_range=range(new_block_cumulative[position-1],threshold)
        
                # Specify all zeros in range of attribute.
                for index in temp_range:
                    temporary[index]=0
                
                # Set one specific column of these zeros to be 1
                temporary[inference_instance]=1
                
                # Append to queries
                queries.append(temporary)
                
                # Generate corresponding evidence input and append
                temporary_evidence_1=["*" for j in range(total_number)]
                temporary_evidence_F1=["*" for j in range(total_number)]
                temporary_evidence_1[5]=0
                temporary_evidence_F1[5]=1
                evidence_male.append(temporary_evidence_1)
                evidence_female.append(temporary_evidence_F1)

import csv

csvfile="dependent_single_attribute_query.q"
male_evidencefile="dependent_protected_attribute_evidence_male.ev"
female_evidencefile="dependent_protected_attribute_evidence_female.ev"

# Write queries and evidence to CSV files for input to SPN Inference task.
with open(csvfile,"w") as output:
    writer=csv.writer(output,lineterminator='\n')
    writer.writerows(queries)

with open(male_evidencefile,"w") as output:
    writer=csv.writer(output,lineterminator='\n')
    writer.writerows(evidence_male)

with open(female_evidencefile,"w") as output:
    writer=csv.writer(output,lineterminator='\n')
    writer.writerows(evidence_female)




